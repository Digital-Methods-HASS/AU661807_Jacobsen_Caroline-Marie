---
title: "Assignment w.12"
author: "Caroline-Marie Jacobsen"
date: "2025-03-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```


### Get the IPCC report:
```{r get-document, results="hide"}
got_path <- here("data","got.pdf")
got_path
got_text <- pdf_text(got_path)
got_text
```

#Texting a single page
```{r single-page, results="hide"}
got_p9 <- got_text[9]
got_p9
```

### Some wrangling:

```{r split-lines, }
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

got_df

```
#Here we are using:  
`stringr::str_split() to split up pages into separate line (separated by `\n`)  
`tidyr::unnest()`to unnest into regular columns  
white space with `stringr::str_trim()`to remove leading/trailing  

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens
```
Now each word has its own row

Then we are counting the words 
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```

### Remove stop words:

We will remove stop words using `tidyr::anti_join()`:
```{r stopwords}
View(stop_words)

got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```

Now check the counts again: 
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
got_swc
```

Get rid of all the numbers (non-text) in `got_stop`
```{r skip-numbers}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of IPCC report words (non-numeric)

```{r wordcloud-prep}
length(unique(got_no_numeric$word))

#The top 100 most frequent
got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
got_top100
```

```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```

Customising
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "star") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

### Sentiment analysis  
"afinn" shows the words ranked from -5 (very negative) to +5 (very positive)

```{r afinn}
get_sentiments(lexicon = "afinn")

#positive words
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

afinn_pos
```

bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```

Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative  
**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```

Let's do sentiment analysis on the GOT text data using afinn, and nrc. 

### Sentiment analysis with afinn: 

First, bind words in `got_stop` to `afinn` lexicon:
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
got_afinn
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# and plotting them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()
```

Investigatig in more depth. Looking at what the words in -2 are. 
```{r afinn-2}
got_afinn2 <- got_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(got_afinn2$word)

# Counting & plotting them
got_afinn2_n <- got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()
```

Or we can summarize sentiment for the report: 
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
got_summary
```
The mean and median indicate negative overall sentiments based on the AFINN lexicon. 

### NRC lexicon for sentiment analysis

As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Check which are excluded using `anti_join()`:

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

Now find some counts: 
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plotting them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

Or count by sentiment and word, then facet:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Showing it
got_nrc_gg

# Saving it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"),
       height = 8, 
       width = 5,
bg = "white")

```

Here we see that "lord" is under both trust and disgust. We will here check in which other sentiments it appears. 
```{r nrc-lord}
lord <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

lord
```
The word "lord" appears in more than one sentiment. Here in both "disgust", "negative", "positive" and "trust". It depends on how one looks at the word and what is represents for each person. In broader perspective, how one looks at the current institution of power, here represented by the lord. 