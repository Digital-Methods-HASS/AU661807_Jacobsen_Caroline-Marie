---
title: "Assignment w.12, Sentiment Analysis"
author: "Caroline Marie Jacobsen"
date: "2025-03-21"
output: html_document
---

## Sentiment Analysis with Game of Thrones 

I will start by loading my libraries below
```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```


### Getting the IPCC report: the got pdf from github 
```{r get-document, results="hide"}
got_path <- here("data","got.pdf")
got_path
got_text <- pdf_text(got_path)
got_text
```

#Texting a single page
```{r single-page, results="hide"}
got_p9 <- got_text[9]
got_p9
```

### Some wrangling:

```{r split-lines, }
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

got_df

```
Here I am are using:  
`stringr::str_split() to split up pages into separate line (separated by `\n`)  
`tidyr::unnest()`to unnest into regular columns  
white space with `stringr::str_trim()`to remove leading/trailing  

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

## Getting the tokens (individual words) in tidy format

Using `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. I am interested in *words*, so that's the token we'll use:

```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens
```
Now each word has its own row

Then I am are counting the words, using the count() function 
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```

### Removing stop words:

I will remove stop words using `tidyr::anti_join()`:
```{r stopwords}
View(stop_words)

got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```

Then I am checking the counts again: 
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
got_swc
```

Getting rid of all the numbers (non-text) in `got_stop`
```{r skip-numbers}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of IPCC report words (non-numeric)

```{r wordcloud-prep}
length(unique(got_no_numeric$word))

#The top 100 most frequent words
got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
got_top100
```
Making the words into a word cloud

```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```

Customising the word cloud with colours and changing the shape into a star
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "star") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

### Sentiment analysis  
Using "afinn" to show the words ranked from -5 (very negative) to +5 (very positive)

```{r afinn}
get_sentiments(lexicon = "afinn")

#positive words
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

afinn_pos
```

Using bing to categorise words into binary categories: "positive" or "negative"

```{r bing}
get_sentiments(lexicon = "bing")
```

Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative  
**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```

Then I am doing sentiment analysis on the GOT text data using afinn, and nrc. 

### Sentiment analysis with afinn: 

First, binding words in `got_stop` to `afinn` lexicon:
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
got_afinn
```

Then finding some counts (by sentiment ranking):
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# and plotting them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()
```

Investigating in more depth. Looking at what the words in -2 are. 
```{r afinn-2}
got_afinn2 <- got_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(got_afinn2$word)

# Counting & plotting them
got_afinn2_n <- got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()
```

Or we can summarize sentiment for the report: 
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
got_summary
```
The mean and median indicate negative overall sentiments based on the AFINN lexicon. 

### NRC lexicon for sentiment analysis

As above, I will use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Check which are excluded using `anti_join()`:

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

Now find some counts: 
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plotting them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

Or count by sentiment and word, then facet:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Showing it
got_nrc_gg

# Saving it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"),
       height = 8, 
       width = 5,
bg = "white")

```

Here we see that "lord" is under both trust and disgust. I will here check in which other sentiments it appears. 

```{r nrc-lord}
lord <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

lord
```
## Explaing the visualisation 

This visualisation shows the words from the Game of Thrones’ text grouped into 10 different sentiments. Overall, the words mostly appear in the negative words category, Ex. anger, sadness, fear. Having read the book I am not surprised by this result. The book contains many violent scenes, often resulting in death.  The visualisations shows the word “death” appearing numerous times and also in different sentiments. The sentiments also shows a difference in the meaning of certain words. An example here would be the word “Lord”.  Lord appears in more than one sentiment. Here in both "disgust", "negative", "positive" and "trust". It depends on how one looks at the word and what is represents for each person. In broader perspective, how one looks at the current institution of power, here represented by the lord. The same however does not apply to the word “king” which only appears in the sentiment “Positive” even though the same logic concerning the view of power could be applied here.  

However, one surprise here is that “death” is not considered a negative word in this sentiment. This is a clear example of what the visualisation lacks. By grouping the words  in these sentiments, some elements gets lost in the process. This becomes important if one were to lead something out of only the two categories “Positive” and “Negative”, where the result will show next to no difference between the count of positive and negative words and thus making it hard to determine just be these categories if the text is mostly positive or negative.  
